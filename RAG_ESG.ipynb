{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fe2f52",
   "metadata": {},
   "source": [
    "# RAG-Enhanced ESG Scoring\n",
    "\n",
    "This notebook implements a Retrieval-Augmented Generation (RAG) approach for ESG scoring, \n",
    "combining the methodologies from the Exercise folder with ESG analysis to overcome token limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be221eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict, Any\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RAG-specific imports (from Exercise folder)\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_ibm import WatsonxLLM, WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.metanames import GenParams\n",
    "\n",
    "# Evaluation imports\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Environment setup\n",
    "from decouple import config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "print(\"RAG-Enhanced ESG Analysis Setup Complete\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae310f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "env_path = \"/Users/wenlong/Documents/GitHub/ma2/assignments/.env\"\n",
    "if os.path.exists(env_path):\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Get API credentials\n",
    "WX_API_KEY = os.getenv(\"WX_API_KEY\")\n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "WX_PROJECT_ID = \"fb3ce137-1a1e-411b-b5f4-d66b00a139f0\"\n",
    "\n",
    "if not WX_API_KEY:\n",
    "    print(\"Warning: WX_API_KEY not found in environment variables\")\n",
    "else:\n",
    "    print(\"✓ Environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b377ddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function (from Exercise/rag.ipynb)\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Basic document preprocessing:\n",
    "    - Normalize Unicode characters\n",
    "    - Remove HTML tags\n",
    "    - Remove unwanted special characters\n",
    "    - Normalize whitespace\n",
    "    - Convert to lowercase\n",
    "    \"\"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)  # Remove HTML\n",
    "    text = re.sub(r\"[^a-zA-Z0-9.,;:!?()\\-\\'\\\"\\n ]+\", \" \", text)  # Clean special chars\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Normalize whitespace\n",
    "    return text.strip().lower()\n",
    "\n",
    "def update_documents_with_headers(chunks):\n",
    "    \"\"\"\n",
    "    Creates a new list of Document objects with page_content prepended with headers\n",
    "    in [Header1/Header2/Header3]: format\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    \n",
    "    for doc in chunks:\n",
    "        # Create a deep copy of the document to avoid modifying the original\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Get all headers that exist in metadata\n",
    "        headers = []\n",
    "        for i in range(1, 4):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(new_doc.metadata[key])\n",
    "        \n",
    "        # Create the header prefix and update page_content\n",
    "        if headers:\n",
    "            prefix = f\"[{'/'.join(headers)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "print(\"✓ Text preprocessing functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7314a347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load benchmark ESG scores\n",
    "def load_benchmark_scores():\n",
    "    \"\"\"Load benchmark ESG scores from benchMark.md\"\"\"\n",
    "    benchmark_scores = {\n",
    "        'Nike': 70, 'Apple': 70, 'Boeing': 79, 'Disney': 68, 'Eli Lilly': 64, 'FedEx': 71,\n",
    "        'Johnson & Johnson': 85, 'JPMorgan Chase': 80, 'McDonald\\'s': 66, 'Meta': 60,\n",
    "        'Microsoft': 87, 'Netflix': 37, 'NVIDIA': 77, 'Tesla': 72, 'Google': 81\n",
    "    }\n",
    "    return benchmark_scores\n",
    "\n",
    "# Load and preprocess company documents\n",
    "def load_and_process_company_documents():\n",
    "    \"\"\"Load all ESG-related documents for each company and prepare for RAG\"\"\"\n",
    "    data_path = \"data\"\n",
    "    company_documents = {}\n",
    "    \n",
    "    # Company directory mapping\n",
    "    company_dirs = {\n",
    "        'Nike': 'nike', 'Apple': 'apple', 'Boeing': 'boeing', 'Disney': 'disney',\n",
    "        'Eli Lilly': 'elililly', 'FedEx': 'fedex', 'Johnson & Johnson': 'johnsonandjohnson',\n",
    "        'JPMorgan Chase': 'jpmorganchase', 'McDonald\\'s': 'mcdonald', 'Meta': 'meta',\n",
    "        'Microsoft': 'microsoft', 'Netflix': 'netflix', 'NVIDIA': 'nvidia',\n",
    "        'Tesla': 'tesla', 'Google': 'google'\n",
    "    }\n",
    "    \n",
    "    print(\"Loading and preprocessing company documents...\")\n",
    "    for company, dir_name in company_dirs.items():\n",
    "        company_path = os.path.join(data_path, dir_name)\n",
    "        if os.path.exists(company_path):\n",
    "            documents = []\n",
    "            file_count = 0\n",
    "            \n",
    "            # Load all relevant files\n",
    "            for file in os.listdir(company_path):\n",
    "                if file.endswith(('.md', '.txt')):\n",
    "                    file_path = os.path.join(company_path, file)\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            \n",
    "                        # Preprocess the content\n",
    "                        cleaned_content = preprocess_text(content)\n",
    "                        \n",
    "                        # Create document object\n",
    "                        doc = Document(\n",
    "                            page_content=cleaned_content,\n",
    "                            metadata={\n",
    "                                \"company\": company,\n",
    "                                \"source_file\": file,\n",
    "                                \"file_path\": file_path\n",
    "                            }\n",
    "                        )\n",
    "                        documents.append(doc)\n",
    "                        file_count += 1\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading {file_path}: {e}\")\n",
    "            \n",
    "            if documents:\n",
    "                company_documents[company] = documents\n",
    "                total_chars = sum(len(doc.page_content) for doc in documents)\n",
    "                print(f\"✓ {company}: {file_count} files, {total_chars} characters\")\n",
    "            else:\n",
    "                print(f\"✗ {company}: No documents found\")\n",
    "        else:\n",
    "            print(f\"✗ {company}: Directory not found - {company_path}\")\n",
    "    \n",
    "    return company_documents\n",
    "\n",
    "# Load data\n",
    "benchmark_scores = load_benchmark_scores()\n",
    "company_documents = load_and_process_company_documents()\n",
    "\n",
    "print(f\"\\nLoaded documents for {len(company_documents)} companies\")\n",
    "print(f\"Benchmark scores available for {len(benchmark_scores)} companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5239ff5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM and embeddings (following Exercise methodology)\n",
    "def setup_rag_components():\n",
    "    \"\"\"Setup LLM and embedding models for RAG\"\"\"\n",
    "    \n",
    "    # Initialize LLM with optimized parameters for ESG scoring\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=\"ibm/granite-13b-instruct-v2\",\n",
    "        url=WX_API_URL,\n",
    "        apikey=WX_API_KEY,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "        params={\n",
    "            GenParams.DECODING_METHOD: \"greedy\",\n",
    "            GenParams.TEMPERATURE: 0,\n",
    "            GenParams.MIN_NEW_TOKENS: 10,\n",
    "            GenParams.MAX_NEW_TOKENS: 500,\n",
    "            GenParams.REPETITION_PENALTY: 1.2\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Initialize embeddings\n",
    "    embeddings = WatsonxEmbeddings(\n",
    "        model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "        url=WX_API_URL,\n",
    "        project_id=WX_PROJECT_ID,\n",
    "        apikey=WX_API_KEY,\n",
    "        params={}\n",
    "    )\n",
    "    \n",
    "    return llm, embeddings\n",
    "\n",
    "# Setup components\n",
    "llm, embeddings = setup_rag_components()\n",
    "print(\"✓ LLM and embeddings initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a09926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document chunking and vector store creation\n",
    "def create_company_vector_stores(company_documents, embeddings):\n",
    "    \"\"\"Create vector stores for each company's documents\"\"\"\n",
    "    \n",
    "    company_retrievers = {}\n",
    "    \n",
    "    # Headers for markdown splitting (if needed)\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"), \n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"), \n",
    "        (\"####\", \"Header 4\")\n",
    "    ]\n",
    "    \n",
    "    # Text splitter with semantic-aware separators\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=60,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "    \n",
    "    print(\"Creating vector stores for each company...\")\n",
    "    \n",
    "    for company, documents in company_documents.items():\n",
    "        try:\n",
    "            # Combine all documents for the company\n",
    "            all_chunks = []\n",
    "            \n",
    "            for doc in documents:\n",
    "                # Split document into chunks\n",
    "                chunks = text_splitter.split_documents([doc])\n",
    "                all_chunks.extend(chunks)\n",
    "            \n",
    "            if all_chunks:\n",
    "                # Create vector store for this company\n",
    "                vector_store = Chroma.from_documents(\n",
    "                    collection_name=f\"esg_{company.lower().replace(' ', '_').replace('&', 'and')}\",\n",
    "                    embedding=embeddings,\n",
    "                    persist_directory=f\"esg_vector_db_{company.lower().replace(' ', '_').replace('&', 'and')}\",\n",
    "                    documents=all_chunks,\n",
    "                )\n",
    "                \n",
    "                # Create retriever\n",
    "                retriever = vector_store.as_retriever(\n",
    "                    search_type=\"similarity\",\n",
    "                    search_kwargs={\"k\": 5}  # Retrieve top 5 relevant chunks\n",
    "                )\n",
    "                \n",
    "                company_retrievers[company] = retriever\n",
    "                print(f\"✓ {company}: {len(all_chunks)} chunks, vector store created\")\n",
    "            else:\n",
    "                print(f\"✗ {company}: No chunks created\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ {company}: Error creating vector store - {e}\")\n",
    "    \n",
    "    return company_retrievers\n",
    "\n",
    "# Create vector stores\n",
    "company_retrievers = create_company_vector_stores(company_documents, embeddings)\n",
    "print(f\"\\nCreated vector stores for {len(company_retrievers)} companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4b7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESG scoring prompt template\n",
    "def create_esg_scoring_template():\n",
    "    \"\"\"Create prompt template for ESG scoring\"\"\"\n",
    "    \n",
    "    template = \"\"\"You are an expert ESG (Environmental, Social, Governance) analyst. Based on the provided company documents, analyze and score the company's ESG performance on a scale of 0-100.\n",
    "\n",
    "Consider the following ESG factors:\n",
    "\n",
    "ENVIRONMENTAL (0-35 points):\n",
    "- Climate change initiatives and carbon footprint reduction\n",
    "- Resource efficiency and waste management\n",
    "- Environmental compliance and sustainability practices\n",
    "- Renewable energy adoption and environmental innovation\n",
    "\n",
    "SOCIAL (0-35 points):\n",
    "- Employee wellbeing, diversity, and inclusion\n",
    "- Community engagement and social impact\n",
    "- Product safety and customer satisfaction\n",
    "- Labor practices and human rights\n",
    "\n",
    "GOVERNANCE (0-30 points):\n",
    "- Board composition and independence\n",
    "- Executive compensation and transparency\n",
    "- Risk management and compliance\n",
    "- Stakeholder engagement and business ethics\n",
    "\n",
    "Company: {company}\n",
    "\n",
    "Relevant Documents:\n",
    "{context}\n",
    "\n",
    "Based on the above information, provide:\n",
    "1. Environmental score (0-35): [score] - [brief justification]\n",
    "2. Social score (0-35): [score] - [brief justification]\n",
    "3. Governance score (0-30): [score] - [brief justification]\n",
    "4. Total ESG Score: [sum of above scores]\n",
    "\n",
    "Only use information from the provided documents. If information is insufficient for any category, provide a conservative estimate and note the limitation.\n",
    "\n",
    "ESG Analysis:\"\"\"\n",
    "    \n",
    "    return PromptTemplate.from_template(template)\n",
    "\n",
    "# Create prompt template\n",
    "esg_prompt = create_esg_scoring_template()\n",
    "print(\"✓ ESG scoring prompt template created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea16b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-based ESG scoring function\n",
    "def score_company_esg_rag(company, retriever, llm, prompt_template):\n",
    "    \"\"\"Score a company's ESG performance using RAG approach\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Retrieve relevant documents for ESG analysis\n",
    "        esg_query = f\"ESG environmental social governance sustainability {company} climate diversity board ethics\"\n",
    "        retrieved_docs = retriever.invoke(esg_query)\n",
    "        \n",
    "        # Combine retrieved documents\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document {i+1}:\\n{doc.page_content}\" \n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "        \n",
    "        # Format prompt\n",
    "        formatted_prompt = prompt_template.invoke({\n",
    "            \"company\": company,\n",
    "            \"context\": context\n",
    "        })\n",
    "        \n",
    "        # Generate ESG analysis\n",
    "        response = llm.invoke(formatted_prompt)\n",
    "        \n",
    "        # Extract total score from response\n",
    "        score = extract_esg_score(response)\n",
    "        \n",
    "        return {\n",
    "            'company': company,\n",
    "            'predicted_score': score,\n",
    "            'analysis': response,\n",
    "            'retrieved_docs_count': len(retrieved_docs)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error scoring {company}: {e}\")\n",
    "        return {\n",
    "            'company': company,\n",
    "            'predicted_score': 50,  # Default middle score\n",
    "            'analysis': f\"Error in analysis: {e}\",\n",
    "            'retrieved_docs_count': 0\n",
    "        }\n",
    "\n",
    "def extract_esg_score(response_text):\n",
    "    \"\"\"Extract the total ESG score from LLM response\"\"\"\n",
    "    \n",
    "    # Look for \"Total ESG Score:\" pattern\n",
    "    patterns = [\n",
    "        r\"Total ESG Score:\\s*(\\d+)\",\n",
    "        r\"Total:\\s*(\\d+)\",\n",
    "        r\"Overall Score:\\s*(\\d+)\",\n",
    "        r\"Final Score:\\s*(\\d+)\",\n",
    "        r\"Score:\\s*(\\d+)\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            score = int(match.group(1))\n",
    "            # Ensure score is within valid range\n",
    "            return max(0, min(100, score))\n",
    "    \n",
    "    # Fallback: look for any number between 0-100\n",
    "    numbers = re.findall(r'\\b(\\d{1,2}|100)\\b', response_text)\n",
    "    if numbers:\n",
    "        # Take the last number found (often the total)\n",
    "        return int(numbers[-1])\n",
    "    \n",
    "    # Default if no score found\n",
    "    return 50\n",
    "\n",
    "print(\"✓ RAG-based ESG scoring functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e72300c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG-based ESG scoring for all companies\n",
    "def run_rag_esg_analysis():\n",
    "    \"\"\"Run RAG-based ESG analysis for all companies\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Running RAG-based ESG analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Process companies that have both retrievers and benchmark scores\n",
    "    companies_to_analyze = set(company_retrievers.keys()) & set(benchmark_scores.keys())\n",
    "    \n",
    "    for company in tqdm(companies_to_analyze, desc=\"Analyzing companies\"):\n",
    "        print(f\"\\nAnalyzing {company}...\")\n",
    "        \n",
    "        # Get retriever for this company\n",
    "        retriever = company_retrievers[company]\n",
    "        \n",
    "        # Score using RAG approach\n",
    "        result = score_company_esg_rag(company, retriever, llm, esg_prompt)\n",
    "        \n",
    "        # Add benchmark score\n",
    "        result['benchmark_score'] = benchmark_scores[company]\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"✓ {company}: Predicted={result['predicted_score']}, Benchmark={result['benchmark_score']}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "rag_results = run_rag_esg_analysis()\n",
    "print(f\"\\nCompleted RAG analysis for {len(rag_results)} companies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4157719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate RAG performance\n",
    "def evaluate_rag_performance(results):\n",
    "    \"\"\"Evaluate the performance of RAG-based ESG scoring\"\"\"\n",
    "    \n",
    "    # Extract predictions and actual scores\n",
    "    y_true = [r['benchmark_score'] for r in results]\n",
    "    y_pred = [r['predicted_score'] for r in results]\n",
    "    companies = [r['company'] for r in results]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Company': companies,\n",
    "        'Benchmark_Score': y_true,\n",
    "        'RAG_Predicted_Score': y_pred,\n",
    "        'Absolute_Error': [abs(t - p) for t, p in zip(y_true, y_pred)]\n",
    "    })\n",
    "    \n",
    "    print(\"RAG-Enhanced ESG Scoring Results\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
    "    print(f\"Root Mean Square Error (RMSE): {rmse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.3f}\")\n",
    "    print(f\"Mean Benchmark Score: {np.mean(y_true):.1f}\")\n",
    "    print(f\"Mean Predicted Score: {np.mean(y_pred):.1f}\")\n",
    "    \n",
    "    return results_df, {'MAE': mae, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "# Evaluate performance\n",
    "results_df, metrics = evaluate_rag_performance(rag_results)\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d925103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RAG performance\n",
    "def visualize_rag_performance(results_df):\n",
    "    \"\"\"Create visualizations for RAG performance\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. Predicted vs Actual scatter plot\n",
    "    axes[0, 0].scatter(results_df['Benchmark_Score'], results_df['RAG_Predicted_Score'], \n",
    "                      alpha=0.7, s=100, color='blue')\n",
    "    axes[0, 0].plot([30, 90], [30, 90], 'r--', label='Perfect Prediction')\n",
    "    axes[0, 0].set_xlabel('Benchmark ESG Score')\n",
    "    axes[0, 0].set_ylabel('RAG Predicted Score')\n",
    "    axes[0, 0].set_title('RAG: Predicted vs Actual ESG Scores')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Error distribution\n",
    "    axes[0, 1].hist(results_df['Absolute_Error'], bins=8, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[0, 1].set_xlabel('Absolute Error')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].set_title('Distribution of Absolute Errors')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Company comparison\n",
    "    x_pos = range(len(results_df))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1, 0].bar([x - width/2 for x in x_pos], results_df['Benchmark_Score'], \n",
    "                   width, label='Benchmark', alpha=0.7, color='orange')\n",
    "    axes[1, 0].bar([x + width/2 for x in x_pos], results_df['RAG_Predicted_Score'], \n",
    "                   width, label='RAG Predicted', alpha=0.7, color='blue')\n",
    "    \n",
    "    axes[1, 0].set_xlabel('Companies')\n",
    "    axes[1, 0].set_ylabel('ESG Score')\n",
    "    axes[1, 0].set_title('Company-wise Score Comparison')\n",
    "    axes[1, 0].set_xticks(x_pos)\n",
    "    axes[1, 0].set_xticklabels(results_df['Company'], rotation=45, ha='right')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Error by company\n",
    "    colors = ['red' if err > 15 else 'orange' if err > 10 else 'green' for err in results_df['Absolute_Error']]\n",
    "    axes[1, 1].bar(range(len(results_df)), results_df['Absolute_Error'], \n",
    "                   color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Companies')\n",
    "    axes[1, 1].set_ylabel('Absolute Error')\n",
    "    axes[1, 1].set_title('Prediction Error by Company')\n",
    "    axes[1, 1].set_xticks(range(len(results_df)))\n",
    "    axes[1, 1].set_xticklabels(results_df['Company'], rotation=45, ha='right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create visualizations\n",
    "visualize_rag_performance(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10468e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG performance with previous approaches\n",
    "def compare_with_previous_results():\n",
    "    \"\"\"Compare RAG results with BOW, BERT, and standard LLM approaches\"\"\"\n",
    "    \n",
    "    # Previous results from the exercise (approximate values from exam report)\n",
    "    previous_results = {\n",
    "        'BOW': {'MAE': 4.65, 'RMSE': 6.12, 'R2': 0.89},\n",
    "        'BERT': {'MAE': 6.30, 'RMSE': 8.45, 'R2': 0.82},\n",
    "        'LLM (Standard)': {'MAE': 21.87, 'RMSE': 24.32, 'R2': 0.15},\n",
    "        'RAG-LLM': metrics\n",
    "    }\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(previous_results).T\n",
    "    \n",
    "    print(\"Performance Comparison Across All Approaches\")\n",
    "    print(\"=\" * 50)\n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Visualize comparison\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    methods = comparison_df.index.tolist()\n",
    "    \n",
    "    # MAE comparison\n",
    "    axes[0].bar(methods, comparison_df['MAE'], color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "    axes[0].set_title('Mean Absolute Error (Lower is Better)')\n",
    "    axes[0].set_ylabel('MAE')\n",
    "    axes[0].tick_params(axis='x', rotation=45)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # RMSE comparison\n",
    "    axes[1].bar(methods, comparison_df['RMSE'], color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "    axes[1].set_title('Root Mean Square Error (Lower is Better)')\n",
    "    axes[1].set_ylabel('RMSE')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # R² comparison\n",
    "    axes[2].bar(methods, comparison_df['R2'], color=['skyblue', 'lightgreen', 'salmon', 'gold'])\n",
    "    axes[2].set_title('R² Score (Higher is Better)')\n",
    "    axes[2].set_ylabel('R²')\n",
    "    axes[2].tick_params(axis='x', rotation=45)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Create comparison\n",
    "comparison_results = compare_with_previous_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dff0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RAG results for further analysis\n",
    "def save_rag_results(results_df, metrics):\n",
    "    \"\"\"Save RAG results to CSV files\"\"\"\n",
    "    \n",
    "    # Save detailed results\n",
    "    results_df.to_csv('esg_scores_rag_analysis.csv', index=False)\n",
    "    print(\"✓ Detailed results saved to 'esg_scores_rag_analysis.csv'\")\n",
    "    \n",
    "    # Save evaluation metrics\n",
    "    metrics_df = pd.DataFrame([metrics])\n",
    "    metrics_df.to_csv('rag_evaluation_results.csv', index=False)\n",
    "    print(\"✓ Evaluation metrics saved to 'rag_evaluation_results.csv'\")\n",
    "    \n",
    "    # Save sample analyses for review\n",
    "    sample_analyses = []\n",
    "    for result in rag_results[:5]:  # Save first 5 detailed analyses\n",
    "        sample_analyses.append({\n",
    "            'Company': result['company'],\n",
    "            'Predicted_Score': result['predicted_score'],\n",
    "            'Benchmark_Score': result['benchmark_score'],\n",
    "            'Analysis': result['analysis']\n",
    "        })\n",
    "    \n",
    "    sample_df = pd.DataFrame(sample_analyses)\n",
    "    sample_df.to_csv('rag_sample_analyses.csv', index=False)\n",
    "    print(\"✓ Sample analyses saved to 'rag_sample_analyses.csv'\")\n",
    "\n",
    "# Save results\n",
    "save_rag_results(results_df, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c686ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis summary and conclusions\n",
    "print(\"RAG-Enhanced ESG Analysis Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Companies analyzed: {len(results_df)}\")\n",
    "print(f\"Mean Absolute Error: {metrics['MAE']:.2f}\")\n",
    "print(f\"R² Score: {metrics['R2']:.3f}\")\n",
    "\n",
    "print(\"\\nKey Findings:\")\n",
    "if metrics['MAE'] < 22:  # Better than standard LLM\n",
    "    print(\"✓ RAG approach improved upon standard LLM performance\")\n",
    "else:\n",
    "    print(\"✗ RAG approach did not improve upon standard LLM\")\n",
    "\n",
    "if metrics['MAE'] < 10:  # Comparable to BERT/BOW\n",
    "    print(\"✓ RAG performance is competitive with traditional ML approaches\")\n",
    "else:\n",
    "    print(\"✗ RAG performance still lags behind traditional ML approaches\")\n",
    "\n",
    "# Best and worst predictions\n",
    "best_prediction = results_df.loc[results_df['Absolute_Error'].idxmin()]\n",
    "worst_prediction = results_df.loc[results_df['Absolute_Error'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest prediction: {best_prediction['Company']} (Error: {best_prediction['Absolute_Error']:.1f})\")\n",
    "print(f\"Worst prediction: {worst_prediction['Company']} (Error: {worst_prediction['Absolute_Error']:.1f})\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "print(\"The RAG approach demonstrates the potential to overcome token limitations\")\n",
    "print(\"in LLM-based ESG analysis by retrieving relevant context dynamically.\")\n",
    "print(\"Performance improvements depend on document quality and retrieval effectiveness.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
