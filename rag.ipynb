{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e2b6b00",
   "metadata": {},
   "source": [
    "# ESG Analysis with Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "This notebook implements a RAG system for Enhanced ESG analysis using:\n",
    "- WatsonxLLM for text generation\n",
    "- IBM Granite embeddings for document encoding  \n",
    "- Chroma vector database for similarity search\n",
    "- ESG documents from corporate 10-K filings and sustainability reports\n",
    "\n",
    "The goal is to overcome the token limitations discovered in the LLM.ipynb notebook and provide more comprehensive ESG scoring capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ce876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import chromadb\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Literal, List\n",
    "from copy import deepcopy\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# LangGraph for building the RAG pipeline\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.types import State\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Watsonx.ai integration\n",
    "from langchain_ibm import WatsonxEmbeddings\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "\n",
    "# Evaluation dependencies \n",
    "from pydantic import BaseModel, Field\n",
    "import instructor\n",
    "import litellm\n",
    "from litellm import completion\n",
    "from instructor import Mode\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d04ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watsonx.ai Configuration\n",
    "WX_API_KEY = os.getenv('WX_API_KEY')\n",
    "WX_PROJECT_ID = os.getenv('WX_PROJECT_ID') \n",
    "WX_API_URL = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "if not WX_API_KEY or not WX_PROJECT_ID:\n",
    "    raise ValueError(\"Missing required environment variables: WX_API_KEY and/or WX_PROJECT_ID\")\n",
    "\n",
    "print(\"✓ Watsonx.ai credentials configured\")\n",
    "\n",
    "# Initialize Watsonx Embeddings with Granite model\n",
    "watsonx_embedding = WatsonxEmbeddings(\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\",\n",
    "    url=WX_API_URL,\n",
    "    apikey=WX_API_KEY,\n",
    "    project_id=WX_PROJECT_ID,\n",
    ")\n",
    "\n",
    "print(\"✓ Watsonx embeddings initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf62540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Watsonx LLM with optimal parameters for ESG analysis\n",
    "generation_parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.GREEDY,\n",
    "    GenParams.MAX_NEW_TOKENS: 2048,  # Increased for detailed ESG analysis\n",
    "    GenParams.TEMPERATURE: 0.1,     # Low temperature for factual consistency\n",
    "    GenParams.TOP_P: 0.9,\n",
    "    GenParams.STOP_SEQUENCES: [\"<|endoftext|>\", \"\\n\\n\\n\"]\n",
    "}\n",
    "\n",
    "# Initialize the Watsonx LLM \n",
    "model = Model(\n",
    "    model_id=ModelTypes.GRANITE_13B_INSTRUCT_V2,\n",
    "    params=generation_parameters,\n",
    "    credentials={\n",
    "        \"apikey\": WX_API_KEY,\n",
    "        \"url\": WX_API_URL\n",
    "    },\n",
    "    project_id=WX_PROJECT_ID\n",
    ")\n",
    "\n",
    "def llm_invoke(prompt):\n",
    "    \"\"\"Helper function to invoke the Watsonx LLM\"\"\"\n",
    "    response = model.generate_text(prompt)\n",
    "    return response\n",
    "\n",
    "print(\"✓ Watsonx LLM configured for ESG analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18e62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ESG documents from the data directory\n",
    "def load_esg_documents():\n",
    "    \"\"\"Load all ESG-related documents from the data directory\"\"\"\n",
    "    documents = []\n",
    "    data_path = Path(\"data\")\n",
    "    \n",
    "    if not data_path.exists():\n",
    "        raise FileNotFoundError(\"Data directory not found. Please ensure ESG documents are in ./data/\")\n",
    "    \n",
    "    # Load 10-K filings (markdown files)\n",
    "    companies = [\"apple\", \"boeing\", \"disney\", \"elililly\", \"fedex\", \"google\", \n",
    "                \"johnsonandjohnson\", \"jpmorganchase\", \"mcdonald\", \"meta\", \n",
    "                \"microsoft\", \"netflix\", \"nike\", \"nvidia\", \"tesla\"]\n",
    "    \n",
    "    for company in companies:\n",
    "        company_path = data_path / company\n",
    "        if company_path.exists():\n",
    "            # Load 10-K items\n",
    "            for item_file in [\"10k_item1.md\", \"10k_item1A.md\", \"10k_item7.md\", \"10k_item7A.md\", \"ESG_Report.md\"]:\n",
    "                file_path = company_path / item_file.replace(\"10k_\", f\"{company}_10k_\")\n",
    "                if not file_path.exists():\n",
    "                    file_path = company_path / item_file  # Try without company prefix\n",
    "                \n",
    "                if file_path.exists():\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                            content = f.read()\n",
    "                            if content.strip():  # Only add non-empty documents\n",
    "                                doc = Document(\n",
    "                                    page_content=content,\n",
    "                                    metadata={\n",
    "                                        \"company\": company,\n",
    "                                        \"document_type\": item_file.replace(\".md\", \"\"),\n",
    "                                        \"source\": str(file_path)\n",
    "                                    }\n",
    "                                )\n",
    "                                documents.append(doc)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Warning: Could not load {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"✓ Loaded {len(documents)} ESG documents from {len(companies)} companies\")\n",
    "    return documents\n",
    "\n",
    "# Load the documents\n",
    "esg_documents = load_esg_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a1491f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document preprocessing and chunking\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text for better embedding quality\"\"\"\n",
    "    import re\n",
    "    import unicodedata\n",
    "    \n",
    "    # Unicode normalization\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    \n",
    "    # Remove HTML tags and special characters\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\-\\(\\)]', ' ', text)\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "def chunk_documents_with_headers(documents: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks while preserving semantic structure\n",
    "    using markdown headers and semantic separators\n",
    "    \"\"\"\n",
    "    # First pass: Extract headers using MarkdownHeaderTextSplitter\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"), \n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "    \n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "    header_chunks = []\n",
    "    \n",
    "    for doc in documents:\n",
    "        try:\n",
    "            # Apply markdown splitting\n",
    "            splits = markdown_splitter.split_text(doc.page_content)\n",
    "            for split in splits:\n",
    "                # Merge metadata from original document\n",
    "                split.metadata.update(doc.metadata)\n",
    "                header_chunks.append(split)\n",
    "        except Exception as e:\n",
    "            # Fallback to original document if markdown splitting fails\n",
    "            print(f\"Markdown splitting failed for {doc.metadata.get('source', 'unknown')}: {e}\")\n",
    "            header_chunks.append(doc)\n",
    "    \n",
    "    # Second pass: Semantic-aware chunking with overlap\n",
    "    token_safe_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,  # Larger chunks for ESG context\n",
    "        chunk_overlap=100,  # Good overlap for continuity\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \":\", \" \", \"\"],  # Semantic boundaries\n",
    "        length_function=len,\n",
    "    )\n",
    "    \n",
    "    safe_chunks = token_safe_splitter.split_documents(header_chunks)\n",
    "    \n",
    "    # Preprocess content for better embeddings\n",
    "    for chunk in safe_chunks:\n",
    "        chunk.page_content = preprocess_text(chunk.page_content)\n",
    "        \n",
    "    # Remove empty chunks\n",
    "    safe_chunks = [chunk for chunk in safe_chunks if chunk.page_content.strip()]\n",
    "    \n",
    "    print(f\"✓ Created {len(safe_chunks)} chunks from {len(documents)} documents\")\n",
    "    return safe_chunks\n",
    "\n",
    "def update_documents_with_headers(chunks: List[Document]) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Add header context to chunk content for better retrieval\n",
    "    \"\"\"\n",
    "    updated_chunks = []\n",
    "    \n",
    "    for doc in chunks:\n",
    "        new_doc = deepcopy(doc)\n",
    "        \n",
    "        # Build header prefix from metadata\n",
    "        headers = []\n",
    "        for i in range(1, 4):\n",
    "            key = f'Header {i}'\n",
    "            if key in new_doc.metadata:\n",
    "                headers.append(new_doc.metadata[key])\n",
    "        \n",
    "        # Add company and document type context\n",
    "        company = new_doc.metadata.get('company', '')\n",
    "        doc_type = new_doc.metadata.get('document_type', '')\n",
    "        \n",
    "        context_parts = []\n",
    "        if company:\n",
    "            context_parts.append(f\"Company: {company}\")\n",
    "        if doc_type:\n",
    "            context_parts.append(f\"Document: {doc_type}\")\n",
    "        if headers:\n",
    "            context_parts.append(f\"Section: {'/'.join(headers)}\")\n",
    "            \n",
    "        if context_parts:\n",
    "            prefix = f\"[{' | '.join(context_parts)}]: \"\n",
    "            new_doc.page_content = prefix + \"\\n\" + new_doc.page_content\n",
    "        \n",
    "        updated_chunks.append(new_doc)\n",
    "    \n",
    "    return updated_chunks\n",
    "\n",
    "# Process the documents\n",
    "print(\"Processing ESG documents...\")\n",
    "processed_chunks = chunk_documents_with_headers(esg_documents)\n",
    "final_docs = update_documents_with_headers(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af901d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create enhanced ESG vector database\n",
    "print(\"Creating enhanced ESG vector database...\")\n",
    "\n",
    "# Initialize Chroma vector database\n",
    "esg_vector_db = Chroma.from_documents(\n",
    "    collection_name=\"enhanced_esg_collection\",\n",
    "    embedding=watsonx_embedding,\n",
    "    persist_directory=\"enhanced_comprehensive_esg_db\",  # Enhanced version\n",
    "    documents=final_docs,\n",
    ")\n",
    "\n",
    "print(f\"✓ Created vector database with {len(final_docs)} document chunks\")\n",
    "\n",
    "# Configure retriever with optimal settings for ESG analysis\n",
    "esg_retriever = esg_vector_db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 8,  # Retrieve more context for comprehensive analysis\n",
    "        \"score_threshold\": 0.3  # Filter out low-similarity results\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ ESG retriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd5f9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval with ESG-specific queries\n",
    "test_queries = [\n",
    "    \"What are Apple's environmental sustainability initiatives?\",\n",
    "    \"How does Boeing address carbon emissions and climate change?\",\n",
    "    \"What diversity and inclusion programs does Microsoft have?\",\n",
    "    \"What are Tesla's governance practices and board composition?\",\n",
    "    \"How does Nike approach sustainable manufacturing?\"\n",
    "]\n",
    "\n",
    "print(\"Testing retrieval system with ESG queries:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for query in test_queries[:2]:  # Test first 2 queries\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    retrieved_docs = esg_retriever.invoke(query)\n",
    "    \n",
    "    for i, doc in enumerate(retrieved_docs[:3]):  # Show top 3 results\n",
    "        company = doc.metadata.get('company', 'Unknown')\n",
    "        doc_type = doc.metadata.get('document_type', 'Unknown')\n",
    "        preview = doc.page_content[:200].replace('\\n', ' ')\n",
    "        \n",
    "        print(f\"{i+1}. Company: {company} | Type: {doc_type}\")\n",
    "        print(f\"   Preview: {preview}...\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce32386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESG-specific prompt templates\n",
    "esg_scoring_template = \"\"\"You are an ESG (Environmental, Social, Governance) analysis expert. Based on the provided corporate documents, analyze the company's ESG performance and provide a numerical score.\n",
    "\n",
    "**Scoring Guidelines:**\n",
    "- Environmental (E): Climate action, resource efficiency, pollution prevention, biodiversity (0-100)\n",
    "- Social (S): Employee welfare, community impact, diversity & inclusion, product safety (0-100)  \n",
    "- Governance (G): Board structure, executive compensation, business ethics, transparency (0-100)\n",
    "\n",
    "**Instructions:**\n",
    "1. Only use information explicitly stated in the provided context\n",
    "2. If information is insufficient for a category, assign a score of 50 (neutral)\n",
    "3. Provide specific evidence from the documents to justify each score\n",
    "4. Be objective and avoid speculation beyond the available evidence\n",
    "\n",
    "**Company:** {company}\n",
    "**ESG Category:** {category}\n",
    "\n",
    "**Context Documents:**\n",
    "{context}\n",
    "\n",
    "**Analysis Required:**\n",
    "Provide a {category} score (0-100) for {company} with detailed justification based on the evidence above.\n",
    "\n",
    "**Response Format:**\n",
    "Score: [numerical score 0-100]\n",
    "Justification: [detailed explanation with specific evidence from documents]\n",
    "\"\"\"\n",
    "\n",
    "general_esg_template = \"\"\"You are an ESG analysis expert. Use the provided corporate documents to answer questions about environmental, social, and governance practices.\n",
    "\n",
    "**Guidelines:**\n",
    "- Only use information explicitly stated in the provided context\n",
    "- If you cannot find relevant information, state \"Information not available in provided documents\"\n",
    "- Cite specific evidence when possible\n",
    "- Be factual and objective\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Context Documents:**\n",
    "{context}\n",
    "\n",
    "**Answer:**\"\"\"\n",
    "\n",
    "# Create prompt templates\n",
    "esg_scoring_prompt = PromptTemplate.from_template(esg_scoring_template)\n",
    "general_esg_prompt = PromptTemplate.from_template(general_esg_template)\n",
    "\n",
    "print(\"✓ ESG prompt templates created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd0af87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RAG state for ESG analysis\n",
    "class ESGRAGState(TypedDict):\n",
    "    \"\"\"State for ESG RAG application\"\"\"\n",
    "    company: str\n",
    "    category: str  # 'Environmental', 'Social', 'Governance', or 'General'\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "    score: int  # ESG score 0-100\n",
    "\n",
    "# Define RAG pipeline functions\n",
    "def retrieve_esg_documents(state: ESGRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"Retrieve relevant ESG documents based on company and category\"\"\"\n",
    "    \n",
    "    # Build search query\n",
    "    company = state.get(\"company\", \"\")\n",
    "    category = state.get(\"category\", \"\")\n",
    "    question = state.get(\"question\", \"\")\n",
    "    \n",
    "    # Construct search query with ESG context\n",
    "    if category in [\"Environmental\", \"Social\", \"Governance\"]:\n",
    "        search_query = f\"{company} {category} {question}\"\n",
    "    else:\n",
    "        search_query = f\"{company} {question}\"\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = esg_retriever.invoke(search_query)\n",
    "    \n",
    "    # Filter documents by company if specified\n",
    "    if company:\n",
    "        filtered_docs = [\n",
    "            doc for doc in retrieved_docs \n",
    "            if doc.metadata.get('company', '').lower() == company.lower()\n",
    "        ]\n",
    "        # If no company-specific docs found, use all retrieved docs\n",
    "        if not filtered_docs:\n",
    "            filtered_docs = retrieved_docs\n",
    "    else:\n",
    "        filtered_docs = retrieved_docs\n",
    "    \n",
    "    return {\"context\": filtered_docs}\n",
    "\n",
    "def generate_esg_response(state: ESGRAGState) -> Dict[str, Any]:\n",
    "    \"\"\"Generate ESG analysis or scoring based on retrieved context\"\"\"\n",
    "    \n",
    "    category = state.get(\"category\", \"General\")\n",
    "    company = state.get(\"company\", \"\")\n",
    "    question = state.get(\"question\", \"\")\n",
    "    context_docs = state.get(\"context\", [])\n",
    "    \n",
    "    # Format context\n",
    "    context_text = \"\\n\\n\".join([\n",
    "        f\"Document {i+1} ({doc.metadata.get('document_type', 'Unknown')}):\\n{doc.page_content}\" \n",
    "        for i, doc in enumerate(context_docs)\n",
    "    ])\n",
    "    \n",
    "    # Choose appropriate prompt template\n",
    "    if category in [\"Environmental\", \"Social\", \"Governance\"] and company:\n",
    "        # ESG scoring mode\n",
    "        formatted_prompt = esg_scoring_prompt.invoke({\n",
    "            \"company\": company,\n",
    "            \"category\": category,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "    else:\n",
    "        # General Q&A mode\n",
    "        formatted_prompt = general_esg_prompt.invoke({\n",
    "            \"question\": question,\n",
    "            \"context\": context_text\n",
    "        })\n",
    "    \n",
    "    # Generate response\n",
    "    response = llm_invoke(formatted_prompt.text)\n",
    "    \n",
    "    # Extract score if in scoring mode\n",
    "    score = None\n",
    "    if category in [\"Environmental\", \"Social\", \"Governance\"]:\n",
    "        try:\n",
    "            # Look for \"Score: XX\" pattern\n",
    "            import re\n",
    "            score_match = re.search(r'Score:\\s*(\\d+)', response)\n",
    "            if score_match:\n",
    "                score = int(score_match.group(1))\n",
    "                score = max(0, min(100, score))  # Clamp to 0-100\n",
    "        except:\n",
    "            score = 50  # Default neutral score\n",
    "    \n",
    "    return {\"answer\": response, \"score\": score}\n",
    "\n",
    "# Build the RAG graph\n",
    "esg_rag_builder = StateGraph(ESGRAGState)\n",
    "esg_rag_builder.add_sequence([retrieve_esg_documents, generate_esg_response])\n",
    "esg_rag_builder.add_edge(START, \"retrieve_esg_documents\")\n",
    "esg_rag_graph = esg_rag_builder.compile()\n",
    "\n",
    "print(\"✓ ESG RAG pipeline built successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fdfda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ESG RAG system\n",
    "def test_esg_scoring(company: str, category: str):\n",
    "    \"\"\"Test ESG scoring for a specific company and category\"\"\"\n",
    "    \n",
    "    result = esg_rag_graph.invoke({\n",
    "        \"company\": company,\n",
    "        \"category\": category,\n",
    "        \"question\": f\"Analyze {company}'s {category.lower()} performance for ESG scoring\"\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Test with sample companies and categories\n",
    "test_cases = [\n",
    "    (\"Apple\", \"Environmental\"),\n",
    "    (\"Tesla\", \"Environmental\"), \n",
    "    (\"Microsoft\", \"Social\"),\n",
    "    (\"Boeing\", \"Governance\")\n",
    "]\n",
    "\n",
    "print(\"Testing ESG RAG System:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample_results = []\n",
    "for company, category in test_cases[:2]:  # Test first 2 cases\n",
    "    print(f\"\\n🏢 Company: {company}\")\n",
    "    print(f\"📊 Category: {category}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = test_esg_scoring(company, category)\n",
    "        score = result.get('score', 'N/A')\n",
    "        answer = result.get('answer', 'No response')\n",
    "        \n",
    "        print(f\"Score: {score}/100\")\n",
    "        print(f\"Analysis: {answer[:300]}...\")\n",
    "        \n",
    "        sample_results.append({\n",
    "            'company': company,\n",
    "            'category': category,\n",
    "            'score': score,\n",
    "            'analysis': answer\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c80fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive ESG scoring for all companies\n",
    "def comprehensive_esg_scoring():\n",
    "    \"\"\"Generate ESG scores for all companies using RAG\"\"\"\n",
    "    \n",
    "    companies = [\"apple\", \"boeing\", \"disney\", \"elililly\", \"fedex\", \"google\", \n",
    "                \"johnsonandjohnson\", \"jpmorganchase\", \"mcdonald\", \"meta\", \n",
    "                \"microsoft\", \"netflix\", \"nike\", \"nvidia\", \"tesla\"]\n",
    "    \n",
    "    categories = [\"Environmental\", \"Social\", \"Governance\"]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Generating comprehensive ESG scores using RAG...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for company in tqdm(companies, desc=\"Processing companies\"):\n",
    "        company_scores = {\"company\": company}\n",
    "        \n",
    "        for category in categories:\n",
    "            try:\n",
    "                result = esg_rag_graph.invoke({\n",
    "                    \"company\": company,\n",
    "                    \"category\": category,\n",
    "                    \"question\": f\"Analyze {company}'s {category.lower()} performance for ESG scoring\"\n",
    "                })\n",
    "                \n",
    "                score = result.get('score', 50)  # Default to neutral if no score\n",
    "                company_scores[f\"{category.lower()}_score\"] = score\n",
    "                \n",
    "                # Store the analysis text (truncated)\n",
    "                analysis = result.get('answer', '')[:500]\n",
    "                company_scores[f\"{category.lower()}_analysis\"] = analysis\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {company} - {category}: {e}\")\n",
    "                company_scores[f\"{category.lower()}_score\"] = 50  # Default score\n",
    "                company_scores[f\"{category.lower()}_analysis\"] = f\"Error: {str(e)}\"\n",
    "        \n",
    "        # Calculate overall ESG score (average)\n",
    "        env_score = company_scores.get('environmental_score', 50)\n",
    "        soc_score = company_scores.get('social_score', 50)\n",
    "        gov_score = company_scores.get('governance_score', 50)\n",
    "        \n",
    "        company_scores['overall_esg_score'] = round((env_score + soc_score + gov_score) / 3, 2)\n",
    "        \n",
    "        results.append(company_scores)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Create a sample scoring function for testing (limited companies)\n",
    "def sample_esg_scoring():\n",
    "    \"\"\"Generate ESG scores for a sample of companies\"\"\"\n",
    "    \n",
    "    sample_companies = [\"apple\", \"tesla\", \"microsoft\", \"boeing\", \"nike\"]\n",
    "    categories = [\"Environmental\", \"Social\", \"Governance\"]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    print(\"Generating sample ESG scores using RAG...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for company in sample_companies:\n",
    "        print(f\"\\n📊 Processing: {company.capitalize()}\")\n",
    "        company_scores = {\"company\": company}\n",
    "        \n",
    "        for category in categories:\n",
    "            try:\n",
    "                result = esg_rag_graph.invoke({\n",
    "                    \"company\": company,\n",
    "                    \"category\": category,\n",
    "                    \"question\": f\"Analyze {company}'s {category.lower()} performance for ESG scoring\"\n",
    "                })\n",
    "                \n",
    "                score = result.get('score', 50)\n",
    "                company_scores[f\"{category.lower()}_score\"] = score\n",
    "                print(f\"   {category}: {score}/100\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   {category}: Error - {e}\")\n",
    "                company_scores[f\"{category.lower()}_score\"] = 50\n",
    "        \n",
    "        # Calculate overall score\n",
    "        env_score = company_scores.get('environmental_score', 50)\n",
    "        soc_score = company_scores.get('social_score', 50)\n",
    "        gov_score = company_scores.get('governance_score', 50)\n",
    "        \n",
    "        overall_score = round((env_score + soc_score + gov_score) / 3, 2)\n",
    "        company_scores['overall_esg_score'] = overall_score\n",
    "        print(f\"   Overall ESG: {overall_score}/100\")\n",
    "        \n",
    "        results.append(company_scores)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✓ ESG scoring functions created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be72156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run sample ESG scoring to test the system\n",
    "print(\"Running sample ESG scoring with RAG system...\")\n",
    "\n",
    "# Execute sample scoring\n",
    "rag_sample_results = sample_esg_scoring()\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "rag_df = pd.DataFrame(rag_sample_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAG ESG SCORING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(rag_df[['company', 'environmental_score', 'social_score', 'governance_score', 'overall_esg_score']])\n",
    "\n",
    "# Save results\n",
    "rag_df.to_csv('esg_scores_rag_sample.csv', index=False)\n",
    "print(f\"\\n✓ Results saved to 'esg_scores_rag_sample.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5a03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare RAG results with previous methods\n",
    "def compare_with_previous_methods():\n",
    "    \"\"\"Compare RAG results with BOW, BERT, and LLM approaches\"\"\"\n",
    "    \n",
    "    # Load previous results if available\n",
    "    previous_files = {\n",
    "        'BOW': 'esg_scores_bow_analysis.csv',\n",
    "        'BERT': 'esg_scores_bert_analysis.csv', \n",
    "        'LLM': 'esg_scores_llm_analysis.csv'\n",
    "    }\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    # RAG results (current)\n",
    "    rag_scores = rag_df.set_index('company')['overall_esg_score'].to_dict()\n",
    "    \n",
    "    print(\"Comparison with Previous Methods:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for method, filename in previous_files.items():\n",
    "        try:\n",
    "            if os.path.exists(filename):\n",
    "                df = pd.read_csv(filename)\n",
    "                if 'overall_esg_score' in df.columns:\n",
    "                    method_scores = df.set_index('company')['overall_esg_score'].to_dict()\n",
    "                    \n",
    "                    # Compare scores for common companies\n",
    "                    for company in rag_scores.keys():\n",
    "                        if company in method_scores:\n",
    "                            comparison_data.append({\n",
    "                                'company': company,\n",
    "                                'method': method,\n",
    "                                'score': method_scores[company]\n",
    "                            })\n",
    "                    \n",
    "                    print(f\"✓ Loaded {method} results: {len(method_scores)} companies\")\n",
    "                else:\n",
    "                    print(f\"⚠ {filename} missing 'overall_esg_score' column\")\n",
    "            else:\n",
    "                print(f\"⚠ {filename} not found\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error loading {method}: {e}\")\n",
    "    \n",
    "    # Add RAG results to comparison\n",
    "    for company, score in rag_scores.items():\n",
    "        comparison_data.append({\n",
    "            'company': company,\n",
    "            'method': 'RAG',\n",
    "            'score': score\n",
    "        })\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        # Create pivot table for better visualization\n",
    "        pivot_df = comparison_df.pivot(index='company', columns='method', values='score')\n",
    "        \n",
    "        print(f\"\\nMethod Comparison (Available Companies):\")\n",
    "        print(\"-\"*50)\n",
    "        print(pivot_df.round(2))\n",
    "        \n",
    "        # Calculate method statistics\n",
    "        method_stats = comparison_df.groupby('method')['score'].agg(['mean', 'std', 'count']).round(2)\n",
    "        print(f\"\\nMethod Statistics:\")\n",
    "        print(\"-\"*30)\n",
    "        print(method_stats)\n",
    "        \n",
    "        return comparison_df, pivot_df\n",
    "    else:\n",
    "        print(\"No previous results found for comparison\")\n",
    "        return None, None\n",
    "\n",
    "# Perform comparison\n",
    "comparison_df, pivot_df = compare_with_previous_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9135b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize RAG performance and comparison\n",
    "def create_rag_visualizations():\n",
    "    \"\"\"Create visualizations for RAG ESG analysis\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('RAG ESG Analysis Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. RAG ESG Scores by Category\n",
    "    categories = ['environmental_score', 'social_score', 'governance_score']\n",
    "    category_means = [rag_df[cat].mean() for cat in categories]\n",
    "    category_labels = ['Environmental', 'Social', 'Governance']\n",
    "    \n",
    "    axes[0,0].bar(category_labels, category_means, color=['green', 'blue', 'orange'], alpha=0.7)\n",
    "    axes[0,0].set_title('RAG: Average ESG Scores by Category')\n",
    "    axes[0,0].set_ylabel('Score (0-100)')\n",
    "    axes[0,0].set_ylim(0, 100)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(category_means):\n",
    "        axes[0,0].text(i, v + 1, f'{v:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Company Rankings (RAG Overall Scores)\n",
    "    company_scores = rag_df.set_index('company')['overall_esg_score'].sort_values(ascending=True)\n",
    "    axes[0,1].barh(range(len(company_scores)), company_scores.values, color='skyblue', alpha=0.7)\n",
    "    axes[0,1].set_yticks(range(len(company_scores)))\n",
    "    axes[0,1].set_yticklabels([c.capitalize() for c in company_scores.index])\n",
    "    axes[0,1].set_title('RAG: Company ESG Rankings')\n",
    "    axes[0,1].set_xlabel('Overall ESG Score')\n",
    "    \n",
    "    # 3. Score Distribution\n",
    "    axes[1,0].hist(rag_df['overall_esg_score'], bins=10, color='lightcoral', alpha=0.7, edgecolor='black')\n",
    "    axes[1,0].set_title('RAG: ESG Score Distribution')\n",
    "    axes[1,0].set_xlabel('Overall ESG Score')\n",
    "    axes[1,0].set_ylabel('Number of Companies')\n",
    "    \n",
    "    # 4. Method Comparison (if available)\n",
    "    if comparison_df is not None and pivot_df is not None:\n",
    "        # Box plot of scores by method\n",
    "        methods = comparison_df['method'].unique()\n",
    "        method_data = [comparison_df[comparison_df['method'] == method]['score'].values for method in methods]\n",
    "        \n",
    "        bp = axes[1,1].boxplot(method_data, labels=methods, patch_artist=True)\n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightcoral']\n",
    "        for patch, color in zip(bp['boxes'], colors[:len(bp['boxes'])]):\n",
    "            patch.set_facecolor(color)\n",
    "        \n",
    "        axes[1,1].set_title('ESG Score Comparison: All Methods')\n",
    "        axes[1,1].set_ylabel('ESG Score')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    else:\n",
    "        axes[1,1].text(0.5, 0.5, 'No comparison data\\navailable', \n",
    "                      ha='center', va='center', transform=axes[1,1].transAxes,\n",
    "                      fontsize=12, style='italic')\n",
    "        axes[1,1].set_title('Method Comparison')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RAG ESG ANALYSIS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Companies analyzed: {len(rag_df)}\")\n",
    "    print(f\"Average Environmental score: {rag_df['environmental_score'].mean():.2f}\")\n",
    "    print(f\"Average Social score: {rag_df['social_score'].mean():.2f}\")\n",
    "    print(f\"Average Governance score: {rag_df['governance_score'].mean():.2f}\")\n",
    "    print(f\"Average Overall ESG score: {rag_df['overall_esg_score'].mean():.2f}\")\n",
    "    print(f\"Score standard deviation: {rag_df['overall_esg_score'].std():.2f}\")\n",
    "    \n",
    "    # Top and bottom performers\n",
    "    top_performer = rag_df.loc[rag_df['overall_esg_score'].idxmax()]\n",
    "    bottom_performer = rag_df.loc[rag_df['overall_esg_score'].idxmin()]\n",
    "    \n",
    "    print(f\"\\nTop ESG performer: {top_performer['company'].capitalize()} ({top_performer['overall_esg_score']:.2f})\")\n",
    "    print(f\"Bottom ESG performer: {bottom_performer['company'].capitalize()} ({bottom_performer['overall_esg_score']:.2f})\")\n",
    "\n",
    "# Create visualizations\n",
    "create_rag_visualizations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e654e77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Evaluation and Token Efficiency Analysis\n",
    "def analyze_rag_efficiency():\n",
    "    \"\"\"Analyze RAG system efficiency and advantages over direct LLM\"\"\"\n",
    "    \n",
    "    print(\"RAG SYSTEM EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Calculate document processing statistics\n",
    "    total_docs = len(esg_documents)\n",
    "    total_chunks = len(final_docs)\n",
    "    avg_chunk_size = sum(len(doc.page_content) for doc in final_docs) / len(final_docs)\n",
    "    \n",
    "    print(f\"Document Processing:\")\n",
    "    print(f\"  • Original documents: {total_docs}\")\n",
    "    print(f\"  • Generated chunks: {total_chunks}\")\n",
    "    print(f\"  • Average chunk size: {avg_chunk_size:.0f} characters\")\n",
    "    print(f\"  • Chunks per document: {total_chunks/total_docs:.1f}\")\n",
    "    \n",
    "    # Estimate token efficiency\n",
    "    total_content_length = sum(len(doc.page_content) for doc in esg_documents)\n",
    "    print(f\"\\nToken Efficiency:\")\n",
    "    print(f\"  • Total content: {total_content_length:,} characters\")\n",
    "    print(f\"  • Estimated tokens: {total_content_length/4:,.0f} (assuming 4 chars/token)\")\n",
    "    print(f\"  • RAG retrieval (k=8): ~{8 * avg_chunk_size/4:.0f} tokens per query\")\n",
    "    print(f\"  • Token reduction: {(1 - (8 * avg_chunk_size)/total_content_length)*100:.1f}%\")\n",
    "    \n",
    "    # Retrieval quality assessment\n",
    "    print(f\"\\nRetrieval Quality:\")\n",
    "    print(f\"  • Retrieval strategy: Similarity search\")\n",
    "    print(f\"  • Embedding model: IBM Granite-278m-multilingual\")\n",
    "    print(f\"  • Vector database: Chroma\")\n",
    "    print(f\"  • Search parameters: k=8, threshold=0.3\")\n",
    "    \n",
    "    return {\n",
    "        'total_docs': total_docs,\n",
    "        'total_chunks': total_chunks,\n",
    "        'avg_chunk_size': avg_chunk_size,\n",
    "        'total_content_length': total_content_length,\n",
    "        'token_reduction': (1 - (8 * avg_chunk_size)/total_content_length)*100\n",
    "    }\n",
    "\n",
    "# Advantages of RAG over Direct LLM\n",
    "def rag_advantages_analysis():\n",
    "    \"\"\"Analyze advantages of RAG over direct LLM approach\"\"\"\n",
    "    \n",
    "    print(\"\\nRAG vs DIRECT LLM COMPARISON\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    advantages = {\n",
    "        \"Token Limitations\": {\n",
    "            \"Direct LLM\": \"Limited to 8192 tokens - cannot process full documents\",\n",
    "            \"RAG\": \"Processes unlimited documents through intelligent retrieval\"\n",
    "        },\n",
    "        \"Content Relevance\": {\n",
    "            \"Direct LLM\": \"Processes entire document even if only small portion relevant\", \n",
    "            \"RAG\": \"Retrieves only most relevant sections for each query\"\n",
    "        },\n",
    "        \"Scalability\": {\n",
    "            \"Direct LLM\": \"Performance degrades with longer context\",\n",
    "            \"RAG\": \"Maintains performance regardless of corpus size\"\n",
    "        },\n",
    "        \"Information Quality\": {\n",
    "            \"Direct LLM\": \"May miss relevant info due to token truncation\",\n",
    "            \"RAG\": \"Finds relevant info across entire document corpus\"\n",
    "        },\n",
    "        \"Cost Efficiency\": {\n",
    "            \"Direct LLM\": \"High token usage for repetitive content processing\",\n",
    "            \"RAG\": \"Lower token usage through targeted retrieval\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for category, comparison in advantages.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(f\"  🔴 Direct LLM: {comparison['Direct LLM']}\")\n",
    "        print(f\"  🟢 RAG: {comparison['RAG']}\")\n",
    "    \n",
    "    return advantages\n",
    "\n",
    "# Run efficiency analysis\n",
    "efficiency_stats = analyze_rag_efficiency()\n",
    "advantages = rag_advantages_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f62ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive ESG Analysis (Optional - Full Dataset)\n",
    "def run_comprehensive_analysis():\n",
    "    \"\"\"\n",
    "    Run complete ESG analysis for all companies using RAG\n",
    "    Note: This may take significant time due to LLM API calls\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"COMPREHENSIVE ESG ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    print(\"This will analyze all 15 companies across all ESG categories\")\n",
    "    print(\"Estimated time: 10-15 minutes\")\n",
    "    print(\"Estimated API calls: ~45 (15 companies × 3 categories)\")\n",
    "    \n",
    "    response = input(\"\\nProceed with comprehensive analysis? (y/n): \")\n",
    "    \n",
    "    if response.lower() == 'y':\n",
    "        print(\"\\nStarting comprehensive ESG analysis...\")\n",
    "        comprehensive_results = comprehensive_esg_scoring()\n",
    "        \n",
    "        # Save results\n",
    "        comprehensive_df = pd.DataFrame(comprehensive_results)\n",
    "        comprehensive_df.to_csv('esg_scores_rag_comprehensive.csv', index=False)\n",
    "        \n",
    "        print(f\"\\n✓ Comprehensive analysis complete!\")\n",
    "        print(f\"✓ Results saved to 'esg_scores_rag_comprehensive.csv'\")\n",
    "        \n",
    "        # Display summary\n",
    "        print(f\"\\nCOMPREHENSIVE RESULTS SUMMARY:\")\n",
    "        print(\"-\"*40)\n",
    "        print(comprehensive_df[['company', 'environmental_score', 'social_score', \n",
    "                               'governance_score', 'overall_esg_score']].round(2))\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(f\"\\nStatistics:\")\n",
    "        print(f\"Mean ESG Score: {comprehensive_df['overall_esg_score'].mean():.2f}\")\n",
    "        print(f\"Std Deviation: {comprehensive_df['overall_esg_score'].std():.2f}\")\n",
    "        print(f\"Min Score: {comprehensive_df['overall_esg_score'].min():.2f}\")\n",
    "        print(f\"Max Score: {comprehensive_df['overall_esg_score'].max():.2f}\")\n",
    "        \n",
    "        return comprehensive_df\n",
    "    else:\n",
    "        print(\"Comprehensive analysis skipped.\")\n",
    "        return None\n",
    "\n",
    "# Interactive Q&A with RAG system\n",
    "def interactive_esg_qa():\n",
    "    \"\"\"Interactive Q&A session with the RAG system\"\"\"\n",
    "    \n",
    "    print(\"\\nINTERACTIVE ESG Q&A\")\n",
    "    print(\"=\"*40)\n",
    "    print(\"Ask questions about ESG practices of any company in the dataset\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"\\nYour question: \")\n",
    "        \n",
    "        if question.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        try:\n",
    "            # Use general Q&A mode\n",
    "            result = esg_rag_graph.invoke({\n",
    "                \"company\": \"\",  # Let retrieval find relevant companies\n",
    "                \"category\": \"General\",\n",
    "                \"question\": question\n",
    "            })\n",
    "            \n",
    "            answer = result.get('answer', 'No response generated')\n",
    "            context_docs = result.get('context', [])\n",
    "            \n",
    "            print(f\"\\nAnswer: {answer}\")\n",
    "            \n",
    "            if context_docs:\n",
    "                print(f\"\\nSources: {len(context_docs)} relevant documents found\")\n",
    "                companies = set(doc.metadata.get('company', 'Unknown') for doc in context_docs[:3])\n",
    "                print(f\"Primary companies: {', '.join(companies)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "print(\"✓ Advanced RAG functions ready\")\n",
    "print(\"\\nOptions:\")\n",
    "print(\"1. run_comprehensive_analysis() - Full analysis of all companies\")\n",
    "print(\"2. interactive_esg_qa() - Interactive Q&A session\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
